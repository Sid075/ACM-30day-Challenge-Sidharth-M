

## Daily Progress

### Cycle 1

| Day   | Summary |
|-------|---------|
| Day 1 | Performed EDA and preprocessing on the Insurance dataset. Handled missing values, encoded categorical features, and visualized key trends. |
| Day 2 | Built regression models (Linear, Decision Tree, Random Forest) on the Insurance dataset. Compared R² scores and trained pipelines. |
| Day 3 | Evaluated model performance using cross-validation. Performed hyperparameter tuning with GridSearchCV for improved accuracy. |
| Day 4 | Explored class imbalance and oversampling techniques. Implemented SMOTE and RandomOverSampler to rebalance the dataset. |
| Day 5 | Visualized feature importance and decision boundaries. Built interactive confusion matrices and classification reports. |

### Cycle 2

| Day   | Summary |
|-------|---------|
| Day 6 | Compared RandomForest, AdaBoost, and XGBoost classifiers on the Breast Cancer dataset using accuracy and basic error analysis. |
| Day 7 | Trained Linear, RBF, and Polynomial SVMs on the Credit Card dataset. Visualized 2D PCA components and compared model accuracy. |
| Day 8 | Clustered the Iris dataset using k-Means and Hierarchical methods. Visualized results with PCA. Explained real-world use cases. |
| Day 9 | Transformed text data using TF-IDF and reduced its dimensionality with SVD. Visualized PCA results and evaluated clustering with silhouette score. |
| Day 10 | Applied KFold cross-validation and plotted learning curves for SVM on the Iris dataset. Analyzed bias-variance tradeoff. |
| Main Challenge | Built a tweet sentiment classifier using the Sentiment140 dataset. Cleaned raw tweets, applied TF-IDF vectorization, and trained an SVM classifier. Evaluated model using accuracy, classification report, and confusion matrix table. |

### Cycle 3

| Phase   | Summary |
|---------|---------|
| Phase 1 | Built a Book Dropout Predictor using a custom Multilayer Perceptron (MLP) in PyTorch. Processed the Goodbooks-10K dataset, engineered numerical features, and predicted whether a user would finish a book. Visualized training loss and evaluated accuracy, precision, recall, and F1-score. |
| Phase 2 | Developed a Facial Emotion Recognition model using a Convolutional Neural Network (CNN) on the FER2013 image dataset. Applied data augmentation, normalization, dropout, and batch normalization. Evaluated accuracy, plotted confusion matrix, and visualized sample correct and incorrect predictions. |

## Repository Contents

- Day1.ipynb – Insurance Dataset EDA  
- Day2.ipynb – Regression Models  
- Day3.ipynb – Cross Validation + GridSearch  
- Day4.ipynb – Handling Imbalanced Data  
- Day5.ipynb – Visualizations & Evaluation  
- Day6.ipynb – Ensemble Classifier Comparison  
- Phase2.ipynb – Support Vector Machines (SVM)  
- Phase3.ipynb – Clustering (Iris Dataset)  
- Phase4.ipynb – Text Clustering with TF-IDF + SVD  
- Phase5.ipynb – Model Validation & Selection with SVM  
- Main_Challenge.ipynb – Tweet Sentiment Analysis using TF-IDF + SVM  
- Phase1.ipynb – Book Dropout Predictor (PyTorch MLP)  
- Phase..2.ipynb – Facial Emotion Recognition (PyTorch CNN)

## Tools and Libraries Used

- Python, Pandas, NumPy  
- Scikit-learn  
- Seaborn and Matplotlib  
- XGBoost, SMOTE, SVD  
- TF-IDF, SVM, GridSearchCV  
- PyTorch  
- torchvision
