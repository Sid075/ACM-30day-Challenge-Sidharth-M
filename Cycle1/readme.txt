ACM 30-Day Machine Learning Challenge – Sidharth M

Cycle 1: Daily Progress

Day 1:
- Performed basic data cleaning and exploratory data analysis (EDA) on a burnout dataset.
- Handled missing values, explored distributions, and generated summary statistics.

Day 2:
- Applied feature encoding and preprocessing techniques.
- Created new interaction features, selected important variables, and prepared the data for modeling.

Day 3:
- Built Logistic Regression and LDA models to classify burnout risk.
- Evaluated models using accuracy, confusion matrix, and ROC-AUC.
- Compared model results and visualized performance with ROC curves.

Day 4:
- Trained Decision Tree, Random Forest, and k-NN models on the burnout dataset.
- Applied feature selection to keep only the top 3 features and compared model accuracy before and after.

Day 5:
- Switched to a regression task using a medical insurance dataset.
- Preprocessed data with label encoding and scaling.
- Trained a Random Forest Regressor to predict insurance charges and achieved high R² performance.
- Used feature importances to identify the top 3 features.
- Built a minimal model using only these 3 features, with only a slight drop in accuracy compared to the full model.

Notebooks:
- Day1.ipynb: Data cleaning and exploration
- Day2.ipynb: Feature encoding and preprocessing
- Day3.ipynb: Classification models and evaluation
- Day4.ipynb: Tree-based models, k-NN, and feature selection
- Day5.ipynb: Random Forest regression and minimal model analysis

Tools & Libraries Used:
- Python (Kaggle Notebook)
- pandas for data handling
- numpy for numerical computations
- scikit-learn for preprocessing, feature selection, and machine learning models
- matplotlib and seaborn for visualization
ACM 30-Day Machine Learning Challenge – Sidharth M

Cycle 1: Daily Progress

Day 1:
- Performed basic data cleaning and exploratory data analysis (EDA) on a burnout dataset.
- Handled missing values, explored distributions, and generated summary statistics.

Day 2:
- Applied feature encoding and preprocessing techniques.
- Created new interaction features, selected important variables, and prepared the data for modeling.

Day 3:
- Built Logistic Regression and LDA models to classify burnout risk.
- Evaluated models using accuracy, confusion matrix, and ROC-AUC.
- Compared model results and visualized performance with ROC curves.

Day 4:
- Trained Decision Tree, Random Forest, and k-NN models on the burnout dataset.
- Applied feature selection to keep only the top 3 features and compared model accuracy before and after.

Day 5:
- Switched to a regression task using a medical insurance dataset.
- Preprocessed data with label encoding and scaling.
- Trained a Random Forest Regressor to predict insurance charges and achieved high R² performance.
- Used feature importances to identify the top 3 features.
- Built a minimal model using only these 3 features, with only a slight drop in accuracy compared to the full model.

Notebooks:
- Day1.ipynb: Data cleaning and exploration
- Day2.ipynb: Feature encoding and preprocessing
- Day3.ipynb: Classification models and evaluation
- Day4.ipynb: Tree-based models, k-NN, and feature selection
- Day5.ipynb: Random Forest regression and minimal model analysis

Tools & Libraries Used:
- Python (Kaggle Notebook)
- pandas for data handling
- numpy for numerical computations
- scikit-learn for preprocessing, feature selection, and machine learning models
- matplotlib and seaborn for visualization
